{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iTp7lUCWvXxE"
      },
      "source": [
        "# Hugging Face Causal Language Model\n",
        "\n",
        "fine-tuning of the [Facebook/opt-125](https://huggingface.co/facebook/opt-125m) model using an portuguese dataset [mc4-pt-sample-1g.txt](https://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt) of 300 million tokens in its causal language modeling pre-training. The opt-125 model was originally trained on a english dataset of approximately 300 bil\\lion tokens.\n",
        "\n",
        "The task is to predict the next token in a sequence, like GPT and not like BERT\n",
        "\n",
        "[![google colab link](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/tcvieira/IA368-DD-012023/blob/main/assingments/04/notebook.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The training is done using an T4 with 16GB of memory\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vsaLBGA0wR87",
        "outputId": "c12d132c-8754-4750-b45b-2fb8db5725c6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Thu Mar 30 00:05:57 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   61C    P8    16W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Installs"
      ],
      "metadata": {
        "id": "FxtL-z4DOlLm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install datasets -q\n",
        "!pip install ipython-autotime -q\n",
        "%load_ext autotime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GR7NS7wUOkjP",
        "outputId": "c612ffc9-b5fe-4cf7-cae2-15f70ce987ee"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 539 µs (started: 2023-03-30 00:06:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "iQ1z2uJwtGUS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    AutoConfig\n",
        ")\n",
        "import torch\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFljHbultHia",
        "outputId": "301df6a5-e650-4f68-b732-3ee2348b2475"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 3
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 5.74 s (started: 2023-03-30 00:06:25 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fv8Y9pDUzriX"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zy064E4bO5V5",
        "outputId": "9efb231c-1167-45c2-d72d-852b47b3ab90"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "time: 6.28 s (started: 2023-03-30 00:06:31 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "PATH_DATASET = '/content/drive/MyDrive/unicamp/IA368DD/class_4'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejSZ8wsKQyp3",
        "outputId": "1ddd9988-ed54-4cea-badb-d585abf82e41"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 483 µs (started: 2023-03-30 00:26:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!gsutil cp gs://unicamp-dl/ia025a_2022s1/aula9/sample-1gb.txt {PATH_DATASET}/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_AOOtmSRbPQ",
        "outputId": "76595bac-1557-4a3e-c43e-4dca9589855c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 190 µs (started: 2023-03-30 00:26:42 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!head {PATH_DATASET}/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlKzW9mxR3Ak",
        "outputId": "3a0597ae-e313-4d96-b94b-cd10c8f8fb9f"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linkbar Há alguns anos, o número de rapazes e moças que subiam ao púlpito para pregar era maior que o de hoje. Na sua simplicidade, falavam do amor de Deus, da Salvação e davam testemunho sob a unção do Espirito Santo. Hoje, parece que a figura do \"preletor oficial\" inibiu muitos de falarem com ousadia a Palavra de Deus. Parece que há um receio de falar diante de um público que, certamente, é mais intelectualizado que há alguns anos. Jovens pregadores ficam embaraçados e cometem certos deslizes, que poderiam ser evitados. Neste modesto trabalho, vamos dar apenas algumas sugestões, e não um estudo sobre a Homilética (Arte de Falar em Publico). I -O QUE PREGAR? É a comunicação verbal da Palavra de Deus aos ouvintes. É a transmissão do evangelho de Nosso Senhor Jesus Cristo às pessoas que precisam ouvi-lo. II- QUAL A FINALIDADE DA PREGAÇÃO? É persuadir as pessoas a aceitarem a mensagem da Palavra de Deus para sua salvação (descrentes) ou para seu crescimento espiritual (crentes). Diante disso, o pregador precisa saber para quem esta falando: Para crentes ou para descrentes? III- QUE DEVE CONTER A PREGAÇÃO? Três coisas são básicas: 1. OBJETIVIDADE. Refere-se ao alvo a atingir. Se pregamos para descrentes, desejamos que eles entendam que precisam crer em Jesus para ser salvos. Devemos orar muito, antes de pregar, para que o Espirito Santo convença as pessoas do seu pecado. Se isso acontecer, a pregação alcança seu alvo. O centro da pregação deve ser Cristo e não o pregador, como acontece em certas cruzadas ou movimentos evangelísticos. Há pregadores que se perdem no púlpito. Começam a falar do amor de Deus, e passam a divagar sobre o Apocalipse, vão até Gênesis, aos profetas e, ao final, não sabem como sair do emaranhado de palavras. É preciso ter objetividade. 2. TRANSMISSÃO. O pregador deve procurar transmitir a mensagem de Deus às pessoas. Paulo disse: \"Porque eu recebi do Senhor o que também vos ensinei...\" O mensageiro deve receber a mensagem de Deus e transmiti-la aos homens. Não deve ficar inventando mensagens, terias, filosofias para mostrar conhecimentos. 3. CONVICÇÃO. O pregador deve transmitir aquilo de que tem convicção, para que a mensagem seja aceita. Tem que viver aquilo que prega. IV - A BASE DA PREGAÇÃO (ou do sermão) 1. A PALAVRA DE DEUS A base da pregação deve ser a Palavra de Deus, a Bíblia Sagrada. Podemos dizer, em outras palavras que a base da pregação deve ser o TEXTO BÍBLICO . Ilustrações podem ser aproveitadas, desde que Que se relacionem com o tema da mensagem , mas não podem tomar o lugar da Palavra de Deus. Ouvimos um pregador que, não tendo êxito em \"abalar\" os ouvintes, apelou para uma história fantasiosa e tomou 80% do tempo destinado à mensagem. 2. QUE TEXTO ESCOLHER? O Pr. Elienai Cabral sugere (em resumo) 8 (oito) características para um bom tema a ser escolhido )p. 50-51). 3) Textos objetivos: que atendam às necessidades espirituais das pessoas (Com oração e unção). 4) Textos sobre os quais não haja dificuldade para a interpretação (hermenêutica). 5) Textos dentro dos limites de capacidade do pregador. 6) Textos que expressem o tema da pregação para não fugir ao objetivo. 7) Texto que desperte interesse (Com oração, o Espírito mostra o que deve ser pregado). 8) Textos cuja seqüência seja de fácil acompanhamento pelo pregador e pelo auditório. V - A ESTRUTURA DA PREGAÇÃO ( Do sermão) Toda pregação com esboço ou não, deve ser dividida, basicamente, em duas partes: 1. INTRODUÇÃO. É a parte inicial da mensagem, pela qual o pregador entra em contato com o auditório. Visa despertar o interesse pela pregação; \"prepara a mente dos ouvintes , para que possam compreender o assunto do sermão e as idéias a serem desenvolvidas...\" (Key, p. 31). Uma boa introdução deve ser BREVE, SIMPLES, INTERESSANTE E APROPRIADA. (Cabral, p. 66) Conhecemos um grande pregador que gasta 30 ou 40 minutos na introdução. Isso cansa, principalmente os descrentes. A introdução não deve ir além de 10 ou 15% do tempo da mensagem. (Normalmente, o pregador sabe de quanto tempo dispõe, exceto em casos especiais). 2. CORPO (ou desenvolvimento) DA MENSAGEM (Do sermão). É a parte mais importante da mensagem. Ela deve conter a seqüência das idéias a serem apresentadas. No corpo do sermão ou da mensagem , podemos ter: 1) Ordem ou divisões (1º , 2º, 3º , etc.); 2) Transição de um pensamento para outro. As divisões devem ser de acordo com os objetivos mensagem; devem-se evitar \" excesso de floreios\", \"rodeios\", ou \"conversa fiada\". O povo percebe. 3.CONCLUSÃO. É o auge da pregação. O seu clímax. Nela, o pregador faz a aplicação do que pregou no corpo do sermão. Nesse momento, o pregador e o auditório, pelo poder do Espirito Santo, devem chegar à conclusão de que a mensagem atingiu seu objetivo. Sem uma boa conclusão, o que foi dito pode perder o brilho. Uma conclusão pode ser feita através de: 1) Recapitulação. O pregador deve rever o que pregou, em resumo ou tópicos, evidenciando pensamentos-chave , pontos fortes da mensagem (Cabral, p. 70). 2) Narração. O pregador pode valer-se de um fato, uma rápida ilustração para comover o auditório, levando o descrente a uma decisão, na unção do Espírito Santo. 3) Persuasão . É a parte mais difícil da conclusão. Depende muito mais do Espírito Santo do que do pregador. Por isso, toda mensagem deve ter a unção do Espírito Santo. Para tanto, o pregador precisa orar muito, e até jejuar, diante de Deus, para que a mensagem atinja seu alvo. 4) Convite. Toda pregação deve terminar com um convite ou apelo, seja para pecadores, seja para a igreja. Um convite na unção do Espírito tem maravilhoso efeito no coração das pessoas. De acordo com Braga (p. 211-212), a conclusão deve ser breve e simples, e com palavras adequadas. Um certo jovem pregou numa igreja. Ao fazer o apelo, não vendo ninguém atender, passou a contar que alguém ganhou um grande prêmio porque deu uma grande oferta para a Obra. Desviou totalmente o alvo da mensagem. VI - TIPOS DE SERMÕES 1. SERMÃO TEMÁTICO (Ou Tópico). É aquele \"cujas divisões principais derivam do tema, independentemente do (Braga, p.17). É aquele em que as divisões principais do derivadas de um TEXTO constituído de UMA BREVE PORÇÃO DA BÍBLIA ( Braga, p. 30). Exemplo: Titulo: \"O Único Caminho Para Deus\" (Jo 14.6). 1) Através de Jesus, o único caminho. 2) Através de Jesus, a verdade. 3)Através de Jesus, a vida. 3. SERMÃO EXPOSITIVO É aquele em que as divisões baseiam-se numa porção mais extensa (texto) da Bíblia, não abrangendo \"um só versículo, mas uma passagem, um capítulo, vários capítulos, ou mesmo um livro inteiro\" (Cabral, p. 78). Nele , é mostrada (exposta) uma verdade contida num texto bíblico. Exige tempo, estudo e conhecimento bíblico. É o que caracteriza uma pessoa e a torna diferente de outra. \"É tudo quanto o indivíduo é\". Na pregação, o pregador demonstra que tem personalidade, quando se expressa, falando ou gesticulando, de acordo com aquilo que ele é e não imitando outras pessoas. De vez em quando, percebe-se pregadores , imitando evangelistas famosos, dando gritos, pulando e correndo no púlpito, torcendo o pescoço, ajeitando a gravata, falando rouco ou estridente. Isso é falta de personalidade. É querer ser ator, imitador e não um instrumento nas mãos do Espírito Santo. É o sentimento de devoção e amor pelos outros e pelas coisas de Deus. O pregador deve sentir pelo Espírito as necessidades do auditório, principalmente dos pecadores. (1 Tm 4.8; Hb 12.28). 2) Devoção É o sentimento religioso, de dedicação às práticas ensinadas na Palavra de Deus. Na devoção, o pregador busca inspirar-se na ORAÇÃO, na LEITURA DA BÍBLIA, e no LOUVAR A DEUS. Temos visto verdadeiros profissionais da pregação, técnicos, que sabem pregar, mas não sabem orar; sabem gritar, mas não sabem amar as almas. Pregam por interesse, por torpe ganância. Que os jovens pregadores (e os antigos) não entrem por esse caminho. Conta-se que Moody, o grande evangelista, orava uma hora para pregar cinco minutos. Enquanto isso, temos pregadores que oram cinco minutos para pregarem uma hora! 3) Sinceridade Reflete a verdade contida na própria alma. O pregador deve pregar aquilo que vive e viver aquilo que prega (Tg 2.12). Um jovem, dirigente de Mocidade, pregava bem. O povo se alegrava. Mas, um dia, uma jovem descrente procurou a direção da igreja para dizer que estava grávida dele e, o pior, o jovem não assumiu a paternidade. Por fim, confessou o pecado, foi excluído, e contribuiu para uma alma descrer do evangelho. 4) Humildade \"Nenhum pregador pode subir ao púlpito sem antes ter descido, pela oração, os degraus da humildade. Na oração, o egoísmo se quebranta. O medo se desfaz, e a certeza da vitória aparece clara como a luz do sol ao meio-dia\" (Cabral, p. 43). (Ler Pv 15.33). Um jovem vivia criticando quem ia pregar, dizendo que, se fosse ele, pregaria muito melhor. Um dia, o pastor deu oportunidade ao moço para pregar. Ele subiu ao púlpito, orgulhoso, sorridente. Tentou achar um texto na Bíblia, de um lado para outro, e nada. Suou, pediu desculpa, e desceu cabisbaixo. Sentou noutro lugar, junto a um irmão experiente, que, percebendo sua tristeza, disse: \"Moço, se você tivesse subido como desceu (humilde), teria descido como subiu (alegre)\". E uma grande lição para todo pregador. 5) Poder O pregador (jovem ou não) precisa do Poder de Deus. S. Paulo disse que não pregava sabedoria humana, mas com poder (1 Co 1.4-5). É preciso ter unção e graça para pregar. Do contrário, ocupa-se o púlpito e o tempo para dizer coisas inoportunas. E melhor um sermão fora da Homilética, mas na unção de Deus, do que dentro da técnica, e sem poder. Isso só se consegue com oração, jejum, leitura bíblica, e vida consagrada. Não se obtém num curso de Homilética. Um mosaico com inscrições em hebraico contando a histórica bíblica de Sansão foi encontrado na Galiléia (Israel), nas ruínas de uma sinagoga do século 4 a.C. A escavação foi conduzida pelo Dr. Jodi Magnees, da North Carolina University (EUA), em parceria com arqueólogos da Israel Antiquities Authority (Autoridade de Antiguidades de Israel). A descoberta ocorreu perto do kibbutz Hokuk, local que várias fontes talmúdicas se referem como sendo a Hokuk histórica, onde rabinos se reuniam para escrever o Talmude (livro sagrado dos judeus). Um livro do rabino mediveal Ashtori Ishtori, do século XIV, faz referência a uma sinagoga na mesma área onde as descobertas foram feitas. Segundo o dr. David Amit, da Autoridade de Antiguidades de Israel, o mosaico \"contém uma descrição do Sansão bíblico e dois pares de raposas com uma tocha flamejante ligando suas caudas\", como descrito nas obras de Sansão no Livro dos Juízes.Read more... \"Não dizeis vós: Ainda há quatro meses até que venha a ceifa? Ora eu vos digo: Levantai os vossos olhos, e vede os campos, que já estão brancos para a ceifa. Quem ceifa já está recebendo recompensa e ajuntando fruto para a vida eterna; para que o que semeia e o que ceifa juntamente se regozijem\" João 4:35,36. Logo após se encontrar com a mulher samaritana e ela ter saído para evangelizar sua cidade, Jesus se encontra com os seus discípulos e estes insistem para que ele coma a comida que eles trouxeram da cidade. Jesus se recusa a comer e passa a ensinar-lhes verdades espirituais sobre o tempo da colheita de Deus. (as vezes precisamos recusar algumas comidas, mesmo que pareçam muito boas e tenham sido trazidas a nós por pessoas com as melhores intenções). Calendários diferentesJesus disse: No calendário de vocês, ainda faltam quatro meses para a colheita, no calendário de Deus, o tempo é agora. É possível que a nossa forma de ver as coisas possa estar diferente da de Deus. Temos discernido que este é um tempo de colheita, mas alguns talvez estejam pensando: Falta muito tempo, não se preocupe, não esquenta a cabeça, temos outras prioridades. Queridos, este é o tempo de Deus para colheita! Com mais ou menos tempo de igreja, com muito preparo ou pouco, com situações pessoais todas resolvidas ou ainda algumas dificuldades para resolver, este é um tempo de colheita e todos poderão participar. A mulher samaritana tinha muitas dificuldades ainda, mas isso não impediu que ela largasse o cântaro e fosse colher vidas para Jesus. Olhos voltados para as coisas terrenasJesus disse: \"Levantai os vossos olhos\", olhos que precisam ser levantados, são olhos que não estão focados onde deveriam estar. Olhos tem relação com o coração. Onde o meu coração está? Nas coisas que desejo? no meu estilo de vida que não pode ser mudado? Nos meus conceitos que são perfeitos e indiscutíveis? Nas coisas terrenas? A recomendação de Jesus é que os nossos olhos estejam nos campos do Senhor, mas somente quem ergue os olhos para olhar firmemente para Jesus é que pode ver e discernir o tempo e perceber que uma grande colheita já está preparada. Mas se o nosso calendário (propósitos e desejos do coração) não estiver alinhado com o do Senhor, nada acontecerá. Nem em sua vida, nem na vida dos que precisam ser alcançados.Semeadura e colheita, só se realizam quando arde no coração o desejo de plantar e colher, quando os olhos do coração estão focalizados no propósito de Deus. Fazendo a coisa certa no lugar certoA ordem de Jesus é semear e colher, não adianta dizer: Senhor, eu trabalho na fazenda, mas só quero cuidar do gado. Em nossa igreja, plantamos e colhemos principalmente através dos grupos familiares, é por isso que líderes não devem somente administrar o grupo ou simplesmente cuidar, mas pastorear e investir plantando coisas do reino de Deus na vida dos seus liderados. O grupo é o celeiro e lugar de treinamento para novos ceifeiros. É o lugar onde recebemos diversos tipos de sementes diferentes ( fé, amor, autoridade, unidade...), traçamos as estratégias apropriadas para plantarmos na vidas das pessoas as coisas do reino de Deus, é onde consideramos a forma de plantio, os adubos a irrigação, os cuidados com as ervas daninhas, é lugar de preparo e encaminhamento (encontro, café com leite puro, discipulado, curso preparatório, veredas, diaconato, treinamento de líderes....) deve haver unidade de propósito e de visão entre os ceifeiros, do contrário a colheita não acontece. Alcançando o resultado finalSemear e colher não acontece por acaso. É necessário um processo. O processo realizado corretamente é o que nos permite chegar ao resultado final. No grupo familiar devemos agir como garimpeiros procurando as pedras preciosas. Primeiro o garimpeiro sonda o terreno, analisa as barrancas do rio, as suas areias, o seu aspecto. Na esperança de que algo de muito valor possa sair dali, ele monta o acampamento (decide investir o seu tempo naquele lugar). Então, entra na água e começa o processo de procura ( pode ser longo). É um processo sistemático mas que deve ser feito com toda atenção ( no descuido podemos perder pedras preciosas). Ele está disposto a enfrentar as dificuldades (friagem no corpo pelo contato com a água fria, dor nas costas, sol na cabeça, picadas de inseto...) desconforto é um preço a ser pago. Mas eis que as pedras começam a surgir na bateia. Hoje uma, daqui a dois meses, duas, depois outras e ele vai encontrando seu tesouro (vidas). Geralmente saem da areia e do barro sujas, sem forma bonita e sem brilho. Mas, logo em seguida começa a fase de lavagem e formação (encontro, libertação, curso preparatório...) e depois a lapidação e polimento (pressões e experiências com Deus). Destino final: Ser parte de uma jóia com muitas outras pedras que refletem a luz da glória de Deus. O local do seu grupo familiar é o canto do riacho que o Senhor te deu para garimpar, as pedras são pessoas esperando para serem alcançadas. Peça a Deus sabedoria, força, direção e entusiasmo para cumprir o propósito. O papa Bento XVI iliba totalmente o povo judeu da morte de Jesus Cristo, um dos assuntos mais controversos do cristianismo, num novo livro de que foram hoje publicados. No livro, intitulado \"Jesus de Nazaré\", Bento XVI recorre a uma análise bíblica e teológica para explicar por que não é verdade que o povo judeu no seu conjunto seja responsável pela morte de Jesus. Embora o Vaticano sustente há cinco décadas que os judeus não foram coletivamente responsáveis, académicos judeus ouvidos pela agência noticiosa norte-americana AP consideraram que o argumento agora exposto pelo papa é significativo e vai contribuir para combater o antissemitismo. \"Há uma tendência humana natural para aceitar as coisas como verdadeiras e muitas vezes isto leva a erros de perceção\" quanto aos riscos de antissemitismo, considerou o rabi David Rosen, responsável para os assuntos inter-religiosos do comité judaico americano que há vários anos lidera o diálogo entre as duas religiões. Segundo Rosen, o Vaticano divulgou em 1965 a sua nota mais autorizada sobre o assunto, \"Nostra Aetate\", nota que revolucionou as relações da Igreja Católica com o judaísmo ao afirmar que a morte de Cristo não pode ser atribuída ao povo judeu nem na altura nem atualmente. Para o rabi, as palavras de Bento XVI podem representar um marco mais importante e duradouro na medida em que os crentes tendem a ler mais as escrituras e artigos ou livros do que os documentos da Igreja, especialmente os mais antigos. Este é o segundo volume de \"Jesus de Nazaré\" de Bento XVI lançado em 2007, o primeiro livro que lançou como papa, sobre os primeiros anos da vida e dos ensinamentos de Jesus Cristo. Este segundo volume, com lançamento previsto para 10 de março, é sobre a segunda parte da vida de Cristo. Quatro cristãos foram presos em Laos por serem surpreendidos pela polícia enquanto tentavam explicar a Bíblia para um homem. Dois deles eram cidadãos tailandeses. O caso aconteceu na aldeia de Luang Namtha na segunda quinzena do mês de junho e só foi divulgado dias depois por agências internacionais. Os relatos afirmam que supostamente um ex-policial ficou chocado por ver esses homens citando a Bíblia e resolveu denunciar. Os presos estavam dentro de uma casa fazendo visitas e falando sobre o Livro Sagrado para o morador. \"Quando a polícia chegou eles procuraram e acharam entre os pertences desses cristãos discos de história da Bíblia e um livro de figuras em preto e branco. A medida da polícia foi tomar os passaportes de todos eles\", disse Prasertsee que é diretor da ONG HRWLRF que defende dos direitos humanos. \"De noite a polícia voltou para prendê-los e os transportaram para a prisão\", continuou Prasertsee. \"Todos os pertences, inclusive dinheiro, fones e coisas pessoas foram confiscados\", denuncia.As famílias estão preocupadas com os prisioneiros, dois dos tailandeses tiveram os nomes divulgados: Phanthakorn, de 40 anos, e Wiwardamrong, de 54. A preocupação se dá pelo fato de que em Laos os prisioneiros não recebem alimento, é a família deles quem fornecem comida. \"As famílias estão se esforçando para ter contato com os presos, mas as autoridades de Laos tem dificultado essa relação\", afirma o direto da HRWLRF. hebraico TRANSLITERAÇÃO DO GREGO É possível fazer a transliteração de qualquer palavra constituída pelo alfabeto grego para o latino e vice-versa. ARQUEOLOGIA E TEOLOGIA pastor paulo Mestrado em Ciências da Religião; Bacharel em Teologia; Psicologia Pastoral; Doutor em Teologia, Doutor Honoris causa em Teologia Sistemática; Psicanálise Clinica com Mestrado em Psicanalise;É bacharel em linguística e hebraico e mestre em hebraico. Professor da área bíblica e de hebraico do Seminário, em São Paulo.\n",
            "Comissão de Construção da Nova Sede GOB. A Comissão de Construção da Nova Sede do Grande Oriente do Brasil - Minas Gerais disponibiliza esta área para que os IIr.'. Maçons, devidamente regularizados, possam acompanhar todo o processo de construção da nova sede do GOB-MG; inclusive esclarecendo possíveis dúvidas que possam existir. Acesse a página da SMI - Sociedade Maçônica de Investimentos S/A e vejam todo conteúdo.\n",
            "06/12/2011 MPT realiza audiência pública e alerta para aumento de acidentes na construção civil 180°.com/PI A construção civil passa por um \"boom\" na cidade de Picos, especialmente na iniciativa privada. Motivado pela necessidade de debater as normas de segurança para os funcionários da construção civil, o Ministério Público do Trabalho do Piauí promoveu na tarde desta sexta-feira (02) uma audiência pública na Câmara Municipal. Além de explanar sobre as normas de segurança a serem seguidas pelos trabalhadores, os procuradores do MPT também alertaram para o aumento de acidentes registrados na construção civil local. \"Agente observa, aqui, que a maioria das obras não se segue as normas mínimas de segurança do trabalho, especialmente a Norma Regulamentada N° 18, que é a norma do Ministério do Trabalho para a construção civil, e o que observamos é um crescente número de acidentes de trabalho e exposição de risco a vida dos trabalhadores\", explicou o procurador do Ministério Público do Trabalho, Carlos Henrique Pereira Leite. Dentre as irregularidades mais graves e constatadas com mais freqüência pelo MPT, estão: trabalho em altura sem proteção e trabalho com instalação elétrica. \"Em que trabalham pessoas não qualificadas e sem a utilização dos equipamentos de proteção individual e coletiva\", informou. Carlos Henrique alerta que o dono de uma obra pode ser responsabilizado caso ocorra um acidente de trabalho. Ele informou que as denúncias ainda são subnotificadas, devido em parte a ausência de um sindicato que lute pelos direitos dos trabalhadores da construção civil em Picos. \"É oportuno lembrar que não há sindicato da construção civil na cidade de Picos\", declarou. A chefe de fiscalização da Superintendência Regional do Trabalho, Soraya Lima declarou que há empenho por parte da Superintendência, mas que esse é um momento de \"boom\" da construção civil em que há um grande número de obras e o consequente aumento das contratações. \"Temos de reconhecer que ainda falta muito para atingir o ideal na construção civil\", declarou. A audiência pública era voltada para os trabalhadores da construção civil e também para os empregadores, oportunidade em que as normas e leis foram explicadas, mas infelizmente a audiência registrou um comparecimento reduzido de pessoas.\n",
            "Usando como exemplo um pneu com código \"P175/70 R14 86V\" já sabemos que é para um veículo de passeio, pois começou com a letra P. Já o código 175/70 é o tamanho do pneu: o número 175 significa a largura de rodagem, medida em milímetros; já o 70 é a altura do pneu, indicando que tem 70% da largura de rodagem – nesse caso 122,5mm. A letra R que vem em seguida significa que o pneu é radial, ou seja, tem uma malha de aço com vergalhões paralelos e transversais. Esses pneus operam sem câmara. O número 14 indicado no código do pneu indica o raio da roda: dependendo do seu veículo, mesmo sendo de passeio, o raio da roda pode ser modificado. Se desejar alterá-lo por qualquer motivo, terá que trocar as rodas também. Por fim, o número 86 significa o índice da carga do pneu, ou seja, ele diz o quanto de peso o pneu aguenta.Aqui, um pneu 86 suporta 530 kgs; como são usados quatro deles num automóvel, ele suporta 2.120 kgs – um peso de carro popular. Existem sites de venda de pneus online que fornecem os tamanhos ideais de aros de pneus conforme a marca do seu carro. Como Cuidar dos Pneus do seu Carro Para que os seus pneus durem mais, é importante mantê-los calibrados no nível correto informado no manual do proprietário, assim como fazer o alinhamento e balanceamento periodicamente. A calibragem deve acontecer a cada 15 dias. Tente memorizar o nível de calibragem para facilitar esse processo. O modo de dirigir também conta: evite aceleradas e freadas bruscas. Vá com calma. Dicas para a Hora de Trocar os Pneus Desgastados Mesmo tomando esses cuidados necessários, eventualmente chega o momento da troca dos pneus por simples desgaste. Uma dica para reduzir o impacto no orçamento é trocá-los aos poucos: compre apenas dois e posteriormente substitua os outros dois. Lembre-se de que o eixo traseiro tem a preferência dos pneus novos. Aproveite e coloque bicos e válvulas novos também. Inovação: Pneus Verdes Recentemente a marca de pneus Pirelli lançou no mercado os chamados \"pneus verdes\". Eles contam com a tecnologia Novateck, desenvolvida especialmente para sua própria reconstrução. Ou seja, ao adquirir o produto, o consumidor, através de reformadores credenciados pela empresa, pode reformar o seu pneu quando desgastes ocorrerem. A marca oferece a mesma qualidade do produto original e garantia de fábrica até a terceira reforma do seu pneu. É um diferencial que a empresa buscar perante os seus clientes e uma forma sustentável de contribuir com o meio ambiente.\n",
            "Utiliza-se quando a ação não ocorre por intervenção deliberada de um agente, mas acidentalmente. Usa-se se + pronome átono + verbo na 3ª pessoa (concordando com o substantivo). ¡Cuidado! ¡El perro se te está escapando! (Cuidado! O cachorro está escapando!) Se me rompió la taza. (A xícara quebrou.) Se nos cayeron los relojes. (Os relógios caíram.) FORMAS ÁTONAS: lo / la / los / las Empregam-se sempre como complemento direto, ou seja, substituindo objetos diretos. 1) Llama un taxi, por favor. (Chama um táxi, por favor.) Llámalo, por favor. (Chama-o, por favor.) 2) Visitaré a mi familia en mis vacaciones. (Visitarei minha família em minhas férias.) La visitaré en mis vacaciones. (Eu a visitarei em minhas férias.) 3) ¿Has encontrado a tus amigos? (Encontraste teus amigos?) No, los estoy buscando. (Não, estou procurando-os.) Atenção! Diferentemente do português, em espanhol os objetos diretos de pessoa ou coisa/animal personificado aparecem precedidos da preposição a. Repare nos exemplos dados acima: os objetos diretos precedidos da preposição a (segundo e terceiro exemplo) são objetos de pessoa, enquanto que o objeto direto não preposicionado (primeiro exemplo) refere-se à coisa não personificada.\n",
            "ATUAÇÃO CINEPASS Uma equipe de alemães está por trás deste novo mercado de vendas de ingressos de cinema. Eles planejam ocupar os 83% de assentos não utilizados nas extremidades traseiras das salas de cinema. Por meio do website, oferecem ofertas exclusivas e preços acessíveis de ingressos e combo de alimentos. Home Página do Filme Página com dados técnicos do filme como Descrição, Duração, Gênero e Elenco. Além das informações sobre o filme, o cliente realiza um filtro de salas de cinema por cidade. Página Promocional Filmes promocionais de determinada sala de cinema. Combos Promocionais O cliente escolhe o combo promocional para determinado filme com data e hora agendada.\n",
            "Agentes de saúde fazem manifestação por melhores condições de trabalho Os agentes de endemias e agentes comunitários da saúde se reuniram hoje pela manhã (17) em uma manifestação na porta da prefeitura reivindicando melhores condições de trabalho. Segundo os representantes do Sindicato dos trabalhadores do Sistema único de Saúde no Estado de Goiás (Sindisaúde) os funcionários cobram melhores salários, além de outros benefícios como vale transporte, equipamentos de proteção individual e uniformes. O sindicato tentou por várias vezes se reunirem com a ex-secretaria de saúde e o prefeito, mas as tentativas não chegaram a lugar algum, o que causou a manifestação. O movimento que começou na porta da prefeitura se estendeu em uma passeata até a Secretária de Saúde, onde estava sendo empossado o novo secretario municipal de saúde. De acordo com Orlando Pedro, um dos cabeça da manifestação, o salário de um agente de endemias é de R$ 779,00, considerado baixo quando relacionado as despesas que cada um tem. \"Tem gente que trabalha em Goiânia e tem que tirar o vale transporte do próprio bolso. Quem fica em áreas mais longes e tem moto, ou carro, também tira do salário para fazer o seu serviço\". Fato este confirmado por uma agente de endemia que diz retirar todo mês do salário mais de R$ 100 só para cumprir sua rota de trabalho. A ex-secretária de saúde, Sônia Maria Martins, disse em uma entrevista de que os funcionários da saúde \"trabalhavam no céu\" e justificou dizendo que até o protetor solar era um dos melhores. Já os agentes discordam da declaração e alegam que poucos recebem o protetor solar, e que o filtro não é o essencial, e trabalham com os mesmos uniformes distribuídos há mais de dois anos. Leocides José Souza, diretor do interior e regional do SindiSaúde, disse que a categoria precisa lutar por um plano de carreira no município e que há uma portaria do Ministério da Saúde que o repasse para os agentes comunitários seria de R$ 950,00 , sendo a diferença entre o salário base e a portaria (algo em torno de R$200) investidos na melhoria de condições dos trabalhadores ou em repasse financeiro como complemento salarial, porém nenhum dos dois é feito na cidade. \"A única coisa que eles dizem ter feito com este valor é comprar algumas bicicletas para os agentes e que todo mês há a manutenção, mas isso não basta. Se há investimentos, prove através de notas que uma melhoria tem sido buscada\", completou o diretor. O prefeito tentou conversar com os agentes antes da manifestação, mas os representantes do sindicato não fariam parte da reunião, o que foi negado. Já no final da passeata, que percorreu as ruas dizendo \"saúde na rua, prefeito a culpa é sua\", Miller Assis conversou com os agentes e disse que tem feito o que a lei permite e o limite do possível para dar condições aos trabalhadores. Ele pediu que as reinvindicações fossem entregues ao novo secretário de saúde, Dirley Côrrea, e que em um prazo breve novas reuniões seriam marcadas. O prefeito disse que o salário dos agentes foi reajustado neste ano, assim como manda a lei e que em um consenso com Conselho Municipal de Saúde decidiu cortas as gratificações recebidas por alguns funcionários, por entender que deve haver isonomia entre os agentes. Em relação aos protetores solares Miller disse que este material já foi comprado, mas não disse quando seria entregue, e que os uniformes estão em fase de licitação. A diretora financeira do Sindisaúde se prontificou a acompanhar as negociações do município e que há outras cidades com menor arrecadação que já pagam o piso salarial de R$ 930,00. Galeria do post Comentários0Comentar!Este artigo ainda não possue comentários. Seja o primeiro a comentar!\n",
            "O governo da província de Misiones, na Argentina, já investiu R$ 13,5 milhões para revitalizar a orla de Puerto Iguazú, às margens do Rio Iguaçu: iniciativa serve de inspiração para o projeto Beira FozFronteira Proposta quer urbanizar orla de Foz Projeto Beira Foz prevê instalação de parques, restaurantes, avenidas e hotéis nas margens do rios Paraná e Iguaçu para combater a criminalidade Exemplo \"Costanera\" trouxe paz e progresso na margem argentina A ocupação das margens dos rios para reduzir a criminalidade e proporcionar bem-estar aos moradores já é uma realidade na Argentina. Em Puerto Iguazú, cidade vizinha a Foz do Iguaçu, o governo da província de Misiones investiu R$ 13,5 milhões para remodelar a orla do rio. A \"costanera\" argentina, como é chamada pelos brasileiros, tem restaurantes, parques, pistas de caminhada e internet gratuita. Omar Rodriguez, diretor de Turismo em Puerto Iguazú, diz que, antes de a orla ser revitalizada, o contrabando de mercadorias do Paraguai via Rio Iguaçu era intenso. \"Agora é paz. A costanera trouxe paz e progresso\", afirma. A orla de Puerto Iguazú é bem menor se comparada à brasileira – são três quilômetros – e a primeira etapa das obras, no trecho de 1.500 metros, está praticamente pronta. Os argentinos aguardam a finalização da construção de restaurantes e bares, um deles já confirmado é o Hard Rock Café em um local cedido pela prefeitura em sistema de concessão pública. Nova etapa Rodriguez adianta que uma segunda etapa da obra está prevista com a ocupação das margens do Rio Iguaçu até a entrada do parque nacional. Entre esses dois pontos já existe uma área de 600 hectares onde está sendo construído um conjunto de hotéis, entre eles, um da rede Hilton. Alguns deles, situados em meio à floresta, já funcionam. A cidade de Posadas, capital de Misiones, tem uma orla exemplar. Situada na fronteira com Encarnación, no Paraguai, a cidade, às margens do Rio Paraná, tem uma extensa orla, com praia artificial, calçadão e restaurantes. Urbanização Novas avenidas aproximam comunidade de rios Um dos aspectos mais importantes do projeto Beira Foz é a reintegração dos moradores à orla. \"Em todo lugar do mundo o rio é o elemento principal para uma cidade, e aqui é o problema. Mas agora vamos resgatar esse potencial\", diz o engenheiro Fábio Prado, representante do Conselho Consultivo da Associação Comercial e Industrial de Foz do Iguaçu (Acifi) e da União Dinâmica de Faculdades Cataratas (UDC) no projeto. Para possibilitar a integração entre a população e os rios, o projeto prevê a construção de avenidas e pistas de caminhada às margens dos Rios Iguaçu e Paraná. A ideia é fazer três traçados. O primeiro, chamado Beira Rio PR-Norte, começa no trevo da Vila C, região da Itaipu Binacional, e termina na Ponte da Amizade. O segundo se estende da ponte até o Marco das Três Fronteiras. E o terceiro liga o marco à entrada do Parque Nacional do Iguaçu. Esse braço do projeto, chamado de Sistema Viário, é coordenado pela Itaipu e pelo Parque Tecnológico de Itaipu (PTI). Entre os locais públicos, estão contemplados no projeto a construção do Parque da Memória, da torre dos 100 anos, em alusão ao centenário de Foz em 2014, a revitalização do Marco das Três Fronteiras, da Colônia de Pescadores, além de pistas para caminhadas e ciclovias. Outros espaços serão ocupados pela iniciativa privada com a construção de hotéis, bares e restaurantes. Aduana Prado lembra que a reurbanização da orla do Rio Paraná tem relação com um movimento iniciado na fronteira em 2001, junto com a Receita Federal, para a ampliação da aduana da Ponte da Amizade, lado brasileiro, cuja obra foi entregue em 2007. Na sequência, a aduana paraguaia também foi reformulada, o que melhorou a receptividade aos turistas nas alfândegas de Foz e Ciudad del Este. Um projeto de iniciativa pública e privada pretende mudar a paisagem da orla do Rio Paraná, na fronteira do Brasil com o Paraguai. Hoje, a área entre a barragem da Itaipu Binacional e o Marco das Três Fronteiras é cercada por portos clandestinos e moradias irregulares, usados pelo contrabando para trazer mercadorias, drogas e armas para o Brasil. A proposta para alterar essa realidade é ocupar os 21 quilômetros de margens do rio com parques, restaurantes, avenidas e hotéis. A ideia – chamada de Projeto Beira Foz – também se estende às margens do Rio Iguaçu, entre o Marco das Três Fronteiras e a entrada do Parque Nacional do Iguaçu, em um trajeto de 17 quilômetros. Além de criar condições para a população de Foz se aproximar e usufruir dos rios, a intervenção urbanística é uma aposta contra a criminalidade. \"O projeto em si é de fundamental importância para mudar não só a cara de Foz do Iguaçu, mas a segurança de todo o país\", opina o delegado-chefe da Polícia Federal (PF) em Foz do Iguaçu, Guilherme de Biagi. Segundo ele, com a ocupação da orla, a própria sociedade se torna um agente fiscalizador. Atualmente, a orla do Paraná tem pelo menos 12 conglomerados de moradias irregulares e uma população de quase mil pessoas. Em meio às residências é comum ver imóveis de fachadas que são depósitos de drogas, cigarros e produtos contrabandeados. O povoamento da beira rio é importante, segundo Biagi, porque a PF pode direcionar o efetivo que hoje patrulha o Rio Paraná para pontos nevrálgicos da fronteira, como o Lago de Itaipu. Ao longo do reservatório, de 1.350 quilômetros, a polícia já mapeou 3 mil miniportos e picadas na mata usados pelos criminosos. Três eixos O Beira Foz está dividido em três eixos: segurança e justiça; habitação e promoção social; e sistema viário. Em relação à segurança, a proposta é instalar três bases ao longo do Rio Paraná chamadas Unidades de Policiamento de Fronteira (UPFron). As estruturas blindadas que serão usadas pela Marinha, PF, Polícia Am­biental e Receita Federal estão sendo projetadas no Parque Tecnológico de Itaipu (PTI), uma das parceiras da proposta. Câmeras para monitoramento eletrônico com visão noturna também auxiliarão o trabalho policial. O projeto envolve os governos federal e estadual, prefeitura de Foz e já tem aval dos Ministérios da Justiça e do Meio Ambiente. Ainda não há estimativa de custo para implementá-lo, mas o valor deve superar R$ 300 milhões. Uma comissão com representantes de instituições públicas e privadas, incluindo a Itaipu Binacional, União Dinâmica de Faculdades Cataratas (UDC) e Associação Comercial e Industrial de Foz do Iguaçu (Acifi), foi formada para levar a proposta adiante. O diretor-superintenden­te da Fundação PTI Juan Sotuyo diz que o Beira Foz vai atrair investimentos privados e beneficiar a cidade. \"O custo benefício é maior que o prejuízo gerado hoje pelo tráfico e contrabando\", ressalta. O mesmo movimento deve ocorrer no Paraguai, segundo Sotuyo. O governo paraguaio acompanha o desenvolvimento do projeto para montar uma estrutura semelhante do outro lado da fronteira, nas cidades de Hernandárias, Ciudad del Este e Presidente Franco, margeadas pelo Rio Paraná. O que você acha O Beira Foz vai conter o trânsito de criminosos na tríplice fronteira? Por quê? Assine a Gazeta do Povo A Cobertura Mais Completa Assine o plano completo da Gazeta do Povo e receba as edições impressas todos os dias da semana + acesso ilimitado no celular, computador e tablet. Tenha a cobertura mais completa do Paraná com a opinião e credibilidade dos melhores colunistas!\n",
            "Ameaçados de punição severa, os ex-palmeirenses Maurício e Obina puderam respirar aliviados, nesta quarta-feira, após o resultado de seus julgamentos no STJD. O zagueiro foi suspenso por três partidas, enquanto o atacante por duas. Os jogadores, que haviam sido denunciados no artigo 253 do Código Brasileiro de Justiça Desportiva (agressão física) por conta da briga em que se envolveram no intervalo da partida entre Palmeiras e Grêmio, no Olímpico em Porto Alegre, pela 36ª rodada do Campeonato Brasileiro, corriam o risco de pegar gancho de 120 a 540 dias. A confusão aconteceu durante uma discussão a respeito do lance que originou o primeiro gol dos gremistas. Antes do início do segundo tempo, ambos foram expulsos e os gaúchos venceram o duelo por 2 a 0. As informações são do site \"Justiça Desportiva\". A defesa de Maurício - que deverá ser emprestado - foi feita pelo advogado do Palmeiras, Rafael Pestana, que negou a agressão e ressaltou o arrependimento do atleta, que teria deixado o estádio chorando. O defensor teve acatado o pedido para que a denúncia fosse desclassificada para o artigo 255 (ato de hostilidade) - que tem pena máxima de três partidas. Obina foi defendido por Michel Assef Filho, advogado do Flamengo - para onde o atacante deverá voltar em 2010. O tribunal desclassificou a denúncia do jogador para o artigo 258 do CBJD (atitude antidesportiva). Assef sustentou que Obina teria reagido a uma ameaça de agressão e usou as imagens para desmentir a súmula do árbitro, afirmando não ter havido troca de socos.\n",
            "Autoridades participam de reabertura dos trabalhos legislativos Os trabalhos legislativos da Assembleia Legislativa de Mato Grosso do Sul foram reabertos na manhã desta terça-feira (2/2), com a solenidade de instalação da 2ª Sessão Legislativa da 10ª Legislatura. O evento, que reúne várias autoridades, teve início às 8h50, no Pavilhão das Bandeiras, entrada principal do Palácio Guaicurus, no Parque dos Poderes. A cerimônia segue um protocolo determinado pelo Regimento Interno da Casa de Leis. Na chegada ao Palácio Guaicurus, o governador Reinaldo Azambuja passou em revista a tropa formada pela Polícia Militar, sendo recebido logo depois pelo presidente Junior Mochi (PMDB) e demais deputados. Prosseguindo o rito, as Bandeiras foram hasteadas. A expectativa com relação a economia foi ressaltada por algumas das autoridades presentes. \"A união existente em nosso Estado, sob o comando do governador Reinaldo e com apoio da Assembleia e Tribunal de Justiça, nos dá esperança que vamos superar a crise econômica e trilhar o melhor caminho possível\", afirmou o desembargador João Maria Lós, presidente do Tribunal de Justiça de Mato Grosso do Sul. Presidente do Tribunal de Contas do Estado, o conselheiro Waldir Neves destacou a importância da relação harmônica entre os Poderes para enfrentar a crise econômica. \"Na atual conjuntura econômica vivida pelo País, os Poderes estão unidos para superarem as dificuldades. Otimismo, garra e vontade são fundamentais. A relação harmônica ajuda muito neste momento\". O Procurador- Geral de Justiça, Humberto de Matos Brites, também comentou sobre relação institucional com a Casa de Leis . \"Sempre tivemos uma relação institucional muito boa com a Assembleia. O Ministério Publico está aqui para desejar um excelente ano legislativo, não obstante a crise que vivemos, temos a certeza que os deputados irão cumprir a risca todas as suas atribuições legais\". Os deputados e autoridades já se dirigiram ao Plenário Júlio Maia para o início da sessão solene. O governador fará a leitura da Mensagem Constitucional de prestação de contas do Poder Executivo referente ao exercício de 2015. Ainda acontecerá o pronunciamento do presidente Junior Mochi e os deputados farão uso da palavra pelo Protocolo. Permitida a reprodução, desde que contenha a assinatura \"Agência ALMS\".Crédito obrigatório para as fotografias, no formato \"Nome do fotógrafo/ALMS\".\n",
            "time: 132 ms (started: 2023-03-30 00:26:43 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l {PATH_DATASET}/sample-1gb.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f_egfjedSUvb",
        "outputId": "363a96fe-b6be-4e01-b8c2-bf92702230b5"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "250000 /content/drive/MyDrive/unicamp/IA368DD/class_4/sample-1gb.txt\n",
            "time: 833 ms (started: 2023-03-30 00:26:44 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## small dataset for testing"
      ],
      "metadata": {
        "id": "ymDm3ZnUVIL8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!sed -n '1,100p' {PATH_DATASET}/sample-1gb.txt > {PATH_DATASET}/sample_small.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyuXMIdoVLGu",
        "outputId": "885d8181-9b48-401f-948c-43e9b197e01f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 231 µs (started: 2023-03-30 00:26:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wc -l {PATH_DATASET}/sample_small.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYKjM8rzVWKe",
        "outputId": "9b7e47f2-7a0f-4eb2-c703-64b54d1a59d0"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100 /content/drive/MyDrive/unicamp/IA368DD/class_4/sample_small.txt\n",
            "time: 128 ms (started: 2023-03-30 00:26:48 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "small_dataset = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample_small.txt')\n",
        "small_dataset[\"validation\"] = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample_small.txt', split=f\"train[:5%]\")\n",
        "small_dataset[\"train\"] = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample_small.txt', split=f\"train[5%:]\")\n",
        "\n",
        "small_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "631c7efd03df4c4a9cb204289eab2eca",
            "e87f1eb1f76c41ca8bbd73ee39c1d805",
            "c0f088987cdb409aab68588995c620d8",
            "66532741e4804fb1943aa1edfbee6396",
            "4162080ad0114a699f4c2c84dce2a548",
            "9c76d023de344be6a40c929bc68d4a65",
            "6b6d3060502447d59855a6b24f23b645",
            "ebaccd3f7a424502afc27c79b88b9aad",
            "cdd95cdaa48c43058866b6ac8b668e5b",
            "c31f9ae10d52499c8aa290ca75d734b8",
            "06817dc1e983454cadd5d5376e258e9e"
          ]
        },
        "id": "lumhT46K0_-H",
        "outputId": "045b8712-2250-4cad-b1db-3465e5f6216f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-9e0a1430f318dd3d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "631c7efd03df4c4a9cb204289eab2eca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-9e0a1430f318dd3d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-9e0a1430f318dd3d/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 95\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 5\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.97 s (started: 2023-03-30 00:26:49 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "base_dataset = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample-1gb.txt')\n",
        "base_dataset[\"validation\"] = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample-1gb.txt', split=f\"train[:20%]\")\n",
        "base_dataset[\"train\"] = load_dataset(\"text\", data_files=f'{PATH_DATASET}/sample-1gb.txt', split=f\"train[20%:]\")\n",
        "\n",
        "base_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295,
          "referenced_widgets": [
            "20fdad5a1ff54cfebba41eb998e1fef4",
            "753ba2211c904b4c9423da6634113473",
            "e315c289bc57403cae77134127a38236",
            "ddbfd3d9a80e4fb88a2ba76794970479",
            "8304186130444d65a4cc0c12ffaaa504",
            "f1215a6d72654114b8fd90e6a1ff6e7a",
            "2f99f22cdf9d46878251ad55dcce7f73",
            "3d92607549134bff9b7ac638c69be201",
            "e7e797818c474004848af949b643511f",
            "32f8504250fb4eb9a39f84ecf62f1b7b",
            "f27a740628d04ebf857721f8aad3828f"
          ]
        },
        "id": "bv_gQCO7pB6G",
        "outputId": "277c284d-9f8f-425c-dfb5-6abf515110d9"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-f6bf8ee78acb5d75/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "20fdad5a1ff54cfebba41eb998e1fef4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-f6bf8ee78acb5d75/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n",
            "WARNING:datasets.builder:Found cached dataset text (/root/.cache/huggingface/datasets/text/default-f6bf8ee78acb5d75/0.0.0/cb1e9bd71a82ad27976be3b12b407850fe2837d80c22c5e03a28949843a8ace2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 200000\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['text'],\n",
              "        num_rows: 50000\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.82 s (started: 2023-03-29 19:56:59 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Select Dataset"
      ],
      "metadata": {
        "id": "hnIQ_w602Oqk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#dataset = small_dataset\n",
        "dataset = base_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5_lth-U22RDw",
        "outputId": "175971d1-ba4d-4812-9172-ee5959559917"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 325 µs (started: 2023-03-29 19:57:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Parameters"
      ],
      "metadata": {
        "id": "6Cv8lozgSeP_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_NAME = 'facebook/opt-125m'\n",
        "MAX_SEQ_LENGTH=1024\n",
        "BATCH_SIZE=4\n",
        "EPOCHS=1\n",
        "MODEL_OUTPUT_FOLDER=f'{PATH_DATASET}/model_output'\n",
        "MODEL_SAVE_FOLDER=f'{PATH_DATASET}/model_save'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uX_3ufzNSgjY",
        "outputId": "8b8b7da8-dbfe-4fe6-f614-a0c9fb3f3b48"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 515 µs (started: 2023-03-29 23:56:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "Zp-IF2AjUqL6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2fDt3xQ3UsjP",
        "outputId": "6ec32c25-2764-42df-bae2-d5b1dcc56926"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.09 s (started: 2023-03-29 23:41:13 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "hi_hABhVcpRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "`AutoModelForCausalLM` is used for auto-regressive language models like all the GPT models, while `AutoModelForSeq2SeqLM` is used for language models with encoder-decoder architecture like T5 and BART.\n",
        "\n",
        "https://huggingface.co/docs/transformers/model_doc/auto#transformers.AutoModelForCausalLM"
      ],
      "metadata": {
        "id": "VZS50t59ccdU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download configuration from huggingface.co and cache.\n",
        "config = AutoConfig.from_pretrained(MODEL_NAME)\n",
        "model = AutoModelForCausalLM.from_config(config)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xxDnD128dBqZ",
        "outputId": "734fda2f-6bfe-462d-8a3d-f8e592b9f178"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OPTForCausalLM(\n",
              "  (model): OPTModel(\n",
              "    (decoder): OPTDecoder(\n",
              "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
              "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
              "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (layers): ModuleList(\n",
              "        (0): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (1): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (2): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (3): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (4): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (5): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (6): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (7): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (8): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (9): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (10): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "        (11): OPTDecoderLayer(\n",
              "          (self_attn): OPTAttention(\n",
              "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
              "          )\n",
              "          (activation_fn): ReLU()\n",
              "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 3.18 s (started: 2023-03-29 23:41:17 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_size = sum(t.numel() for t in model.parameters())\n",
        "print(f\"OPT-125m size: {model_size/1000**2:.1f}M parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xFyDe3w2uT5",
        "outputId": "4f54d388-81a9-4769-be1b-07afb7fc4efe"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OPT-125m size: 125.2M parameters\n",
            "time: 1.16 ms (started: 2023-03-29 19:57:45 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization\n",
        "\n",
        "The model was pretrained with max sequence length of 2048. We use length 256 initially to overcome the exceeded ram problem.\n",
        "\n",
        "CHANGE: we tokenize all samples in the batch (consisting of 1000 documents) and create a long sequence of tokens by concatenating all examples and separating them with the special EOS token. Finally, we divide the long sequence into chunks of 512 tokens, which will be used for training."
      ],
      "metadata": {
        "id": "JSUXfiRMjM15"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_dataset = dataset.map(lambda x: tokenizer(x[\"text\"], \n",
        "                                      truncation=True, \n",
        "                                      padding=\"max_length\", \n",
        "                                      max_length=MAX_SEQ_LENGTH), \n",
        "                                      batched=True, \n",
        "                                      num_proc=4, \n",
        "                                      remove_columns=[\"text\"]\n",
        "                                     )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34,
          "referenced_widgets": [
            "82c8caba97564447915ae880f9313c7e",
            "e1cd1a0b26cb4a2f95604971cdac7469",
            "7b30a43c51d749f88cc0b4544c055614",
            "adaf5e9c80804dbda9251e52f51322bb",
            "51a779f11a0f4135aa6f3f8c36fe1724",
            "08613890264a45df8e1b9bd2472b11df",
            "0b54c6f3f36a4b5bb202cd95afa1f50b",
            "60753e6d3a9f498a892e0db94ab7295f",
            "b83ee0e3f47d409c8198b8901bfe4939",
            "0b1be4c8af244a3fa98e8e747753f9cb",
            "f7f5dbbf87fa43e590f88b757a0f7b0c",
            "a718754ad5394fc78e225841ecdb5927",
            "b0393bd14b3a489bad4bf5199710efe2",
            "ee5e6e6667eb4fc2a4db7f3d52b8739b",
            "07c8bd403c664306a6a8bda614abacbf",
            "479892d5bc17430492e05485b29cfc4d",
            "f10a79ad82174e7087c0a52e7753267c",
            "afb1c51250464d6b82acf3b4e36f3a13",
            "98207b5b89a840cd8d17e72ac87ac728",
            "09d4c2c5bbf849168241bc8486e97894",
            "e367ee1c2922448cbd6e598fc1ea6dd3",
            "ee3d5913a071404992f57a7aa33872b9"
          ]
        },
        "id": "L7cvziurjOof",
        "outputId": "64a036bb-8afd-46f0-a3cd-8fca82a1cc58"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/200000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82c8caba97564447915ae880f9313c7e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map (num_proc=4):   0%|          | 0/50000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a718754ad5394fc78e225841ecdb5927"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 7min 2s (started: 2023-03-29 23:41:41 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenized_dataset)\n",
        "print(f\"{len(tokenized_dataset['train']['input_ids'][0])} tokens - {tokenized_dataset['train']['input_ids'][0]}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KUbw9TEc2bdY",
        "outputId": "f9a8b03f-7e79-4cb2-aa3a-cecdd2b4ebdc"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 200000\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['input_ids', 'attention_mask'],\n",
            "        num_rows: 50000\n",
            "    })\n",
            "})\n",
            "1024 tokens - [2, 10926, 260, 43598, 897, 5441, 2102, 263, 5716, 9803, 109, 17614, 242, 4, 4838, 10870, 16738, 842, 1855, 424, 424, 62, 260, 991, 9401, 4, 673, 32868, 636, 1977, 642, 1020, 842, 3304, 14537, 2953, 290, 5352, 6, 466, 6301, 40632, 364, 8541, 8604, 3137, 316, 361, 4531, 10870, 16738, 117, 952, 3070, 7984, 11332, 740, 225, 2527, 4, 83, 32709, 808, 1829, 4410, 2154, 338, 1526, 506, 2426, 7935, 263, 501, 6, 466, 10870, 16738, 2953, 6301, 40632, 117, 38064, 1479, 12834, 109, 32868, 636, 1977, 642, 1020, 4, 846, 1210, 13265, 9958, 32868, 636, 1977, 642, 4544, 263, 2884, 102, 5874, 6, 22595, 9618, 211, 3181, 12, 22686, 4168, 2102, 939, 4746, 139, 9803, 6, 3105, 260, 8557, 842, 2662, 4324, 10, 3191, 6301, 10, 17074, 12, 673, 13967, 263, 6331, 257, 10, 9131, 7375, 740, 808, 1829, 13736, 4709, 2050, 4765, 4, 104, 15998, 2102, 10, 3492, 1145, 3985, 263, 22917, 6, 263, 3105, 260, 8557, 25, 46521, 225, 13920, 5473, 2154, 338, 1526, 506, 31895, 109, 32868, 636, 1977, 642, 1020, 9882, 11964, 35, 195, 12938, 2843, 108, 2107, 17809, 17074, 21001, 11964, 35, 2908, 12938, 379, 108, 974, 17809, 384, 13967, 4, 673, 1198, 7068, 4842, 263, 3105, 260, 8557, 842, 1855, 2583, 1127, 661, 6658, 5547, 1885, 4, 2884, 102, 7, 417, 281, 25, 4828, 808, 4216, 32626, 415, 20559, 6, 28312, 5563, 181, 4636, 10209, 6534, 1198, 7068, 405, 3851, 263, 3105, 260, 8557, 248, 4324, 3889, 4214, 2659, 6, 1814, 11705, 28312, 5563, 326, 3146, 1140, 119, 181, 4636, 8541, 18416, 10, 1198, 7068, 405, 3851, 2953, 8327, 506, 1264, 11188, 139, 842, 5521, 2544, 242, 295, 5874, 2089, 139, 4832, 36, 6232, 43, 2107, 1096, 12, 30583, 83, 9578, 139, 98, 7805, 1021, 32868, 636, 1977, 642, 1020, 155, 4, 134, 1589, 195, 3840, 1021, 3105, 260, 8557, 23, 6472, 5739, 230, 808, 1829, 12, 387, 5079, 718, 4, 175, 4, 3809, 2841, 361, 117, 548, 6648, 1001, 1125, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "time: 2min 27s (started: 2023-03-29 23:49:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "52rYOmiEjPAc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(output_dir=MODEL_OUTPUT_FOLDER,\n",
        "                                  num_train_epochs=EPOCHS, \n",
        "                                  per_device_train_batch_size=BATCH_SIZE,\n",
        "                                  per_device_eval_batch_size=BATCH_SIZE, \n",
        "                                  evaluation_strategy=\"epoch\", \n",
        "                                  save_strategy=\"epoch\",\n",
        "                                  logging_strategy=\"epoch\", \n",
        "                                  learning_rate=2e-5, \n",
        "                                  weight_decay=0.01,\n",
        "                                  fp16=True # Use mixed precision\n",
        "                                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_xJic3-DjQ5y",
        "outputId": "aeb001dd-efb8-415a-e458-fc292e09781d"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.84 ms (started: 2023-03-29 23:56:47 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "trainer = Trainer(model=model, \n",
        "                  args=training_args, \n",
        "                  train_dataset=tokenized_dataset[\"train\"],\n",
        "                  eval_dataset=tokenized_dataset[\"validation\"], \n",
        "                  data_collator=data_collator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAPtAMhbsGia",
        "outputId": "9c81ba3e-0bab-4bad-f8ae-4f4163ba65a5"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 6.39 ms (started: 2023-03-29 23:56:51 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "id": "TBvfrNQgsY1k",
        "outputId": "53f61ffa-5236-40a3-a899-f05660a70b3c"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-80-3435b262f1ae>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1631\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_training_loop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_train_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_find_batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1632\u001b[0m         )\n\u001b[0;32m-> 1633\u001b[0;31m         return inner_training_loop(\n\u001b[0m\u001b[1;32m   1634\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1635\u001b[0m             \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1900\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1901\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1902\u001b[0;31m                     \u001b[0mtr_loss_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1904\u001b[0m                 if (\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2643\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2644\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2645\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2647\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_gpu\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    694\u001b[0m                 )\n\u001b[1;32m    695\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    697\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# Self Attention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         hidden_states, self_attn_weights, present_key_value = self.self_attn(\n\u001b[0m\u001b[1;32m    327\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m             \u001b[0mpast_key_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0mattn_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattn_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlayer_head_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36msoftmax\u001b[0;34m(input, dim, _stacklevel, dtype)\u001b[0m\n\u001b[1;32m   1839\u001b[0m         \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_softmax_dim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"softmax\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_stacklevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1840\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1841\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1842\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1843\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 192.00 MiB (GPU 0; 14.75 GiB total capacity; 13.60 GiB already allocated; 142.81 MiB free; 13.82 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 87.4 ms (started: 2023-03-29 23:57:07 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(MODEL_SAVE_FOLDER)\n",
        "tokenizer.save_pretrained(MODEL_SAVE_FOLDER)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zn-6bzd9se62",
        "outputId": "8e87dbef-cfb3-49bf-8315-b33fe26f2f34"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/vocab.json',\n",
              " '/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/merges.txt',\n",
              " '/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/added_tokens.json',\n",
              " '/content/drive/MyDrive/unicamp/IA368DD/class_4/model_save/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 54
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 2.72 s (started: 2023-03-29 22:20:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!zip model {MODEL_SAVE_FOLDER}/*"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psdbomF3vxjs",
        "outputId": "e967a97b-f271-425d-b84d-7b4646db61ad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 283 µs (started: 2023-03-29 22:55:03 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ],
      "metadata": {
        "id": "KEd2tpNEjRLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(MODEL_SAVE_FOLDER)\n",
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset['train'],\n",
        "    eval_dataset=tokenized_dataset['validation'],\n",
        "    data_collator = data_collator\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H_PNQljudHI",
        "outputId": "73b6c255-bb36-441d-81ac-c435baa32965"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 1.86 s (started: 2023-03-29 22:48:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate()\n",
        "perplexity = torch.exp(torch.tensor(eval_results[\"eval_loss\"]))\n",
        "print(f'Train Perplexity =  {perplexity.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "qDpjxNFksaPI",
        "outputId": "929a084c-776d-4900-930f-244d1868fda8"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='12500' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12500/12500 06:36]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Perplexity =  31.67\n",
            "time: 6min 36s (started: 2023-03-29 22:48:26 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_results = trainer.evaluate(tokenized_dataset[\"validation\"])\n",
        "perplexity = torch.exp(torch.tensor(eval_results[\"eval_loss\"]))\n",
        "print(f'Test Perplexity =  {perplexity.item():.2f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 384
        },
        "id": "3nEjQiAb260X",
        "outputId": "a3faef02-3a55-4788-bd8b-bfb0aa8e5199"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='73638' max='12500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [12500/12500 49:38]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-11742354c107>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0meval_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokenized_dataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mperplexity\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eval_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Test Perplexity =  {perplexity.item():.2f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   2930\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2931\u001b[0m         \u001b[0meval_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_loop\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_legacy_prediction_loop\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluation_loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2932\u001b[0;31m         output = eval_loop(\n\u001b[0m\u001b[1;32m   2933\u001b[0m             \u001b[0meval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2934\u001b[0m             \u001b[0mdescription\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Evaluation\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mevaluation_loop\u001b[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001b[0m\n\u001b[1;32m   3111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3112\u001b[0m             \u001b[0;31m# Prediction step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3113\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprediction_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprediction_loss_only\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3114\u001b[0m             \u001b[0minputs_decode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_inputs_for_metrics\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mprediction_step\u001b[0;34m(self, model, inputs, prediction_loss_only, ignore_keys)\u001b[0m\n\u001b[1;32m   3366\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mhas_labels\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mloss_without_labels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3367\u001b[0m                     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss_context_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3368\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3369\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3370\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mcompute_loss\u001b[0;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[1;32m   2675\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2676\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2677\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2678\u001b[0m         \u001b[0;31m# Save past state if it exists\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2679\u001b[0m         \u001b[0;31m# TODO: this needs to be fixed and made cleaner later.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    928\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m         \u001b[0;31m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 930\u001b[0;31m         outputs = self.model.decoder(\n\u001b[0m\u001b[1;32m    931\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    932\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    694\u001b[0m                 )\n\u001b[1;32m    695\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 696\u001b[0;31m                 layer_outputs = decoder_layer(\n\u001b[0m\u001b[1;32m    697\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/transformers/models/opt/modeling_opt.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, output_attentions, use_cache, past_key_value)\u001b[0m\n\u001b[1;32m    350\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 25min 46s (started: 2023-03-29 23:12:18 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'rapazes e moças que subiam ao púlpito'\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Generate\n",
        "generate_ids = model.generate(inputs.input_ids.to(device), max_length=30)\n",
        "tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uRgUhhsHfNhl",
        "outputId": "3f7605f5-780b-4e6a-d1ec-761f4dc76436"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'rapazes e moças que subiam ao púlpito, ao público, ao púb'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 63
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "time: 123 ms (started: 2023-03-29 23:38:10 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Leonardo Augusto da Silva Pacheco - https://github.com/leonardo3108/IA368dd/blob/main/exercicios/Aula_5/Aula_5_Treino_Modelo_de_Linguagem.ipynb\n",
        "\n",
        "prompt = 'rapazes e moças que subiam ao púlpito'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenizer(text=prompt)['input_ids']\n",
        "    input_ids_truncated = input_ids[-MAX_SEQ_LENGTH:]  # Usamos apenas os últimos tokens como entrada para o modelo.\n",
        "    output = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = output['logits'][:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    predicted_id = torch.argmax(logits).item()  # extraindo o token de maior probabilidade (greedy decoding)\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt.replace('</s>', ''))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7jJYwfrvmHI",
        "outputId": "78ac08bc-19e1-4c7c-9d21-8a0a632186d4"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rapazes e moças que subiam ao púlpito,\n",
            "rapazes e moças que subiam ao púlpito, a\n",
            "rapazes e moças que subiam ao púlpito, ao\n",
            "rapazes e moças que subiam ao púlpito, ao p\n",
            "rapazes e moças que subiam ao púlpito, ao pú\n",
            "rapazes e moças que subiam ao púlpito, ao púb\n",
            "rapazes e moças que subiam ao púlpito, ao públic\n",
            "rapazes e moças que subiam ao púlpito, ao público\n",
            "rapazes e moças que subiam ao púlpito, ao público,\n",
            "rapazes e moças que subiam ao púlpito, ao público, a\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao p\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao pú\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao púb\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao públic\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao público\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao público de\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao público de j\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao público de jF\n",
            "rapazes e moças que subiam ao púlpito, ao público, ao público de jFC\n",
            "time: 336 ms (started: 2023-03-29 23:38:14 +00:00)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results\n",
        "\n",
        "split train/validation (200.000, 50.000)\n",
        "\n",
        "| seq length | epochs | batch size |  ppl  |\n",
        "|:----------:|:------:|:----------:|:-----:|\n",
        "|     256    |    1   |      4     | 31.67 |\n",
        "|    1024    |    1   |      8     |       |\n",
        "|            |        |            |       |"
      ],
      "metadata": {
        "id": "b4hGBK9uwKXf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----"
      ],
      "metadata": {
        "id": "Hl3CwFerSHyS"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "631c7efd03df4c4a9cb204289eab2eca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e87f1eb1f76c41ca8bbd73ee39c1d805",
              "IPY_MODEL_c0f088987cdb409aab68588995c620d8",
              "IPY_MODEL_66532741e4804fb1943aa1edfbee6396"
            ],
            "layout": "IPY_MODEL_4162080ad0114a699f4c2c84dce2a548"
          }
        },
        "e87f1eb1f76c41ca8bbd73ee39c1d805": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c76d023de344be6a40c929bc68d4a65",
            "placeholder": "​",
            "style": "IPY_MODEL_6b6d3060502447d59855a6b24f23b645",
            "value": "100%"
          }
        },
        "c0f088987cdb409aab68588995c620d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ebaccd3f7a424502afc27c79b88b9aad",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cdd95cdaa48c43058866b6ac8b668e5b",
            "value": 1
          }
        },
        "66532741e4804fb1943aa1edfbee6396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c31f9ae10d52499c8aa290ca75d734b8",
            "placeholder": "​",
            "style": "IPY_MODEL_06817dc1e983454cadd5d5376e258e9e",
            "value": " 1/1 [00:00&lt;00:00, 48.25it/s]"
          }
        },
        "4162080ad0114a699f4c2c84dce2a548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c76d023de344be6a40c929bc68d4a65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b6d3060502447d59855a6b24f23b645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ebaccd3f7a424502afc27c79b88b9aad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdd95cdaa48c43058866b6ac8b668e5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c31f9ae10d52499c8aa290ca75d734b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06817dc1e983454cadd5d5376e258e9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "20fdad5a1ff54cfebba41eb998e1fef4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_753ba2211c904b4c9423da6634113473",
              "IPY_MODEL_e315c289bc57403cae77134127a38236",
              "IPY_MODEL_ddbfd3d9a80e4fb88a2ba76794970479"
            ],
            "layout": "IPY_MODEL_8304186130444d65a4cc0c12ffaaa504"
          }
        },
        "753ba2211c904b4c9423da6634113473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f1215a6d72654114b8fd90e6a1ff6e7a",
            "placeholder": "​",
            "style": "IPY_MODEL_2f99f22cdf9d46878251ad55dcce7f73",
            "value": "100%"
          }
        },
        "e315c289bc57403cae77134127a38236": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3d92607549134bff9b7ac638c69be201",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e7e797818c474004848af949b643511f",
            "value": 1
          }
        },
        "ddbfd3d9a80e4fb88a2ba76794970479": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32f8504250fb4eb9a39f84ecf62f1b7b",
            "placeholder": "​",
            "style": "IPY_MODEL_f27a740628d04ebf857721f8aad3828f",
            "value": " 1/1 [00:00&lt;00:00, 35.78it/s]"
          }
        },
        "8304186130444d65a4cc0c12ffaaa504": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f1215a6d72654114b8fd90e6a1ff6e7a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f99f22cdf9d46878251ad55dcce7f73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d92607549134bff9b7ac638c69be201": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e7e797818c474004848af949b643511f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "32f8504250fb4eb9a39f84ecf62f1b7b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f27a740628d04ebf857721f8aad3828f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82c8caba97564447915ae880f9313c7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1cd1a0b26cb4a2f95604971cdac7469",
              "IPY_MODEL_7b30a43c51d749f88cc0b4544c055614",
              "IPY_MODEL_adaf5e9c80804dbda9251e52f51322bb"
            ],
            "layout": "IPY_MODEL_51a779f11a0f4135aa6f3f8c36fe1724"
          }
        },
        "e1cd1a0b26cb4a2f95604971cdac7469": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_08613890264a45df8e1b9bd2472b11df",
            "placeholder": "​",
            "style": "IPY_MODEL_0b54c6f3f36a4b5bb202cd95afa1f50b",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "7b30a43c51d749f88cc0b4544c055614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_60753e6d3a9f498a892e0db94ab7295f",
            "max": 200000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b83ee0e3f47d409c8198b8901bfe4939",
            "value": 200000
          }
        },
        "adaf5e9c80804dbda9251e52f51322bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b1be4c8af244a3fa98e8e747753f9cb",
            "placeholder": "​",
            "style": "IPY_MODEL_f7f5dbbf87fa43e590f88b757a0f7b0c",
            "value": " 200000/200000 [05:39&lt;00:00, 628.76 examples/s]"
          }
        },
        "51a779f11a0f4135aa6f3f8c36fe1724": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "08613890264a45df8e1b9bd2472b11df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b54c6f3f36a4b5bb202cd95afa1f50b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "60753e6d3a9f498a892e0db94ab7295f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b83ee0e3f47d409c8198b8901bfe4939": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0b1be4c8af244a3fa98e8e747753f9cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7f5dbbf87fa43e590f88b757a0f7b0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a718754ad5394fc78e225841ecdb5927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b0393bd14b3a489bad4bf5199710efe2",
              "IPY_MODEL_ee5e6e6667eb4fc2a4db7f3d52b8739b",
              "IPY_MODEL_07c8bd403c664306a6a8bda614abacbf"
            ],
            "layout": "IPY_MODEL_479892d5bc17430492e05485b29cfc4d"
          }
        },
        "b0393bd14b3a489bad4bf5199710efe2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f10a79ad82174e7087c0a52e7753267c",
            "placeholder": "​",
            "style": "IPY_MODEL_afb1c51250464d6b82acf3b4e36f3a13",
            "value": "Map (num_proc=4): 100%"
          }
        },
        "ee5e6e6667eb4fc2a4db7f3d52b8739b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_98207b5b89a840cd8d17e72ac87ac728",
            "max": 50000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_09d4c2c5bbf849168241bc8486e97894",
            "value": 50000
          }
        },
        "07c8bd403c664306a6a8bda614abacbf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e367ee1c2922448cbd6e598fc1ea6dd3",
            "placeholder": "​",
            "style": "IPY_MODEL_ee3d5913a071404992f57a7aa33872b9",
            "value": " 50000/50000 [01:22&lt;00:00, 603.11 examples/s]"
          }
        },
        "479892d5bc17430492e05485b29cfc4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": "hidden",
            "width": null
          }
        },
        "f10a79ad82174e7087c0a52e7753267c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afb1c51250464d6b82acf3b4e36f3a13": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "98207b5b89a840cd8d17e72ac87ac728": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d4c2c5bbf849168241bc8486e97894": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e367ee1c2922448cbd6e598fc1ea6dd3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ee3d5913a071404992f57a7aa33872b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}